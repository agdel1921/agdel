# -*- coding: utf-8 -*-
"""
Created on Sun Oct 28 14:18:35 2018

@author: LatizeExpress
"""

"""
This file is to prepare some generic code for NTUC data validation and manipulation
This file is generated by Ivy Yi on 20180817
"""

__author__ = 'Latize'
import os
import sys
import time
import pandas as pd
import matplotlib.pyplot as plt
import csv
os.environ['SPARK_HOME']=r"C:\opt\spark\spark-2.3.1-bin-hadoop2\spark-2.3.1-bin-hadoop2.7"
sys.path.append(r"C:\opt\spark\spark-2.3.1-bin-hadoop2\spark-2.3.1-bin-hadoop2.7\python")
from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from pyspark.sql.types import IntegerType
from pyspark.sql.types import FloatType
from functools import reduce
from pyspark.sql.functions import substring
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import Correlation
from pyspark.ml.stat import ChiSquareTest
from pyspark.sql.functions import col
from pyspark.mllib.stat import Statistics
import datetime
from pyspark.sql.types import *
from pyspark.sql.functions import udf
from pyspark.sql import functions as F
conf = SparkConf().setMaster("local").setAppName("NTUC")
sc = SparkContext(conf = conf)
sqlContext=SQLContext(sc)

def read_file(path):
    df = sqlContext.read.csv(path,header=True)
    return (df)

def clean_header(data,oldColumns,newColumns):
    df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), data)
    return(df)

def change_to_int(data,col):
    for conv_col in col:
        data = data.withColumn(conv_col, data[conv_col].cast(IntegerType()))
    return(data)

def change_to_float(data,col):
    for conv_col in col:
        data = data.withColumn(conv_col, data[conv_col].cast(FloatType()))
    return(data)

def change_to_month(data,col):
    for conv_col in col:
        data = data.withColumn(conv_col, data[conv_col].substr(1, 7))
    return(data)

def data_exploration(data,data_name):
    categorical_data=pd.DataFrame()
    descriptive_data=pd.DataFrame()

    categorical_output_loc = r"E:/NTUC/Processed/Categorical_output_/" + data_name + ".csv"
    descriptive_output_loc = r"E:/NTUC/Processed/desc_output_/" + data_name + ".csv"
    for column in data.schema.names:
        graph_name=r"E:/NTUC/Processed/Graph_chart_/" + data_name + "_" + column + ".png"
        flag=data.schema[column].dataType
        categorical_update=data.groupby(column).count().toPandas()
        categorical_update=categorical_update.sort_values(by="count",ascending=False).reset_index(drop=True)
        categorical_data=pd.concat([categorical_data, categorical_update], axis=1, sort=False)
        if isinstance(flag,IntegerType) or isinstance(flag,FloatType):
            descriptive_update=data.describe(column).toPandas()
            descriptive_data = pd.concat([descriptive_data, descriptive_update], axis=1, sort=False)
            categorical_update_graph = categorical_update
            categorical_update_graph.set_index(column, inplace=True)
            plot = categorical_update_graph.plot.bar()
            fig = plot.get_figure()
            fig.savefig(graph_name)
    categorical_data.to_csv(categorical_output_loc, header=True, index=False)
    descriptive_data.to_csv(descriptive_output_loc, header=True, index=False)
    return()

def unique_counts(sampledata):
    path = r"E:/NTUC/Processed/UniqueOutput_/" + "unique" + ".csv"
    for i in sampledata.columns:
        sample = sampledata.toPandas()
        count = sample[i].nunique()
        print(i,": ", count)
        return()


def contentAnalysis(sampleData):
    sample12 = sampleData.toPandas()
    abc = sample12.corr(method='pearson')
    print(abc)
    path = r"E:/NTUC/Processed/correlation/"+"correlation" + ".csv"
    abc.to_csv(path,header=True, index=False)
    return()


## Reading files in pyspark data frame

path = "E:/NTUC/raw_data/Final/Aff_Prem_weight_final.csv"
path1 = "E:/NTUC/raw_data/Final/PremiumWeight/premiumWeight.csv"
vehage = "E:/NTUC/raw_data/Final/PremiumWeight/vehAge.csv"
edulavel = "E:/NTUC/raw_data/Final/education_weight/edu_weight.csv"
vehBrand = "E:/NTUC/raw_data/Final/BrandScore/vehBrandScore.csv"
travel = "E:/NTUC/raw_data/Final/countryScore/CountryScore.csv"
dwel = "E:/NTUC/raw_data/Final/dwellingscore/dwellingweight.csv"
post = "E:/NTUC/raw_data/Final/PostalC0odeWeight/postalScore.csv"
path2 = "E:/NTUC/raw_data/Final/FinalScore_v1.csv"
product = "E:/NTUC/raw_data/Final/productScore.csv"
vehtype = "E:/NTUC/raw_data/Final/vehtype_score/VehTypeScore.csv"


df = pd.read_csv(path, low_memory = True)
df.columns
len(df)
df1 = pd.read_csv(path1, low_memory = True)
df1.columns
veh = pd.read_csv(vehage, low_memory = True)
veh.columns
edu = pd.read_csv(edulavel, low_memory = True)
edu.columns
vhbrand = pd.read_csv(vehBrand, low_memory = True)
vhbrand.columns
country = pd.read_csv(travel, low_memory = True)
country.columns
dweltp = pd.read_csv(dwel, low_memory = True)
dweltp.columns
postal = pd.read_csv(post, low_memory = True)
postal.columns
res = pd.read_csv(path2, low_memory = True)
res.columns
prod = pd.read_csv(product, low_memory = True)
prod.columns
veht = pd.read_csv(vehtype, low_memory = True)
veht.columns
# PremiumScore
result = df.merge(df1, on='premiumbucketaffl', how ='left'  )
result.columns

# VehAge
result = result.merge(veh, on='premiumbucketvehage', how ='left'  )
result.columns
#result.head(5)
len(result)

# Education
result = result.merge(edu, on='educationlevel', how ='left'  )
result.columns
#result.head(5)
len(result)

# VehBrand
result = result.merge(vhbrand, on='vehbrand', how ='left'  )
result.columns
#result.head(5)
len(result)


# Country
result = result.merge(country, on='travelcountry', how ='left'  )
result.columns
#result.head(5)
len(result)

# DwellingType
result = result.merge(dweltp, on='addr_type_desc', how ='left'  )
result.columns
#result.head(5)
len(result)

# PostalCode
result = res.merge(postal, on='PostalCode', how ='left'  )
result.columns

# ProductScore
result = res.merge(prod, on='productseqid', how ='left'  )
result.columns

# Vehtype Score
result = result.merge(veht, on='vehtypename', how ='left'  )
result.columns
len(result)

#CalculatingFinalScore: Vehicle
result['FinalVehScore'] = result['FinalScore_Vehage']+result['FinalScoreVehVrand']+result['TotalScore']
# Final Area Score : DwellingType and Area together


result['FinalPremiumWeight'] = result['FinalScorePremium']*0.45
result['FinalAreaWeight'] = result['FinalAreaScore']*0.30
result['FinalVehWeight'] = result['FinalVehScore']*0.05
result['FinalcountryWeight'] = result['FinalScoreCountry']*0.06
result['FinalEducationWeight'] = result['TotalEdu']*0.30
result['FinalProductWeight'] = result['FinalScore_product']*0.30
result['FinalAffluenceScore'] = result['FinalPremiumWeight'] + result['FinalAreaWeight'] + result['FinalVehWeight'] + result['FinalcountryWeight'] + result['FinalEducationWeight'] + result['FinalProductWeight']



path_out = "E:/NTUC/raw_data/Final/results_affluence/"

#result.head(5)
len(result)
result.to_csv(path_out+'Affluence_final_weight.csv', header =True, index = False)

#Data Exploration
product_path=r"E:/NTUC/raw_data/Final/results_affluence/Affluece_Final_v1.csv"
prod_test=read_file(product_path)

data_exploration(prod_test,"FinalResult_Affluence")











